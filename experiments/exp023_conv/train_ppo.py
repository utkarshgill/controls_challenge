"""
PPO on Conv BC baseline (65.63)
Target: <45 (30% improvement)
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import random, numpy as np, torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from tqdm import trange
from tinyphysics import TinyPhysicsModel, TinyPhysicsSimulator, CONTROL_START_IDX, COST_END_IDX, DEL_T, LAT_ACCEL_COST_MULTIPLIER

device = torch.device('cpu')

BASE_SCALE = np.array([0.3664, 7.1769, 0.1396, 38.7333], dtype=np.float32)
CURV_SCALE = 0.1573

all_files = sorted(Path('./data').glob('*.csv'))
random.seed(42)
random.shuffle(all_files)
train_files, val_files, test_files = all_files[:15000], all_files[15000:17500], all_files[17500:20000]

model_onnx = TinyPhysicsModel('./models/tinyphysics.onnx', debug=False)

max_epochs, routes_per_epoch, eval_interval = 100, 50, 2
batch_size, K_epochs, lr = 4000, 10, 1e-4  # Lower LR to avoid explosion
gamma, gae_lambda, eps_clip, entropy_coef = 0.99, 0.95, 0.2, 0.01

class ActorCritic(nn.Module):
    def __init__(self):
        super().__init__()
        
        # Shared conv for both actor and critic
        self.conv = nn.Sequential(
            nn.Conv1d(1, 16, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.Conv1d(16, 32, kernel_size=5, padding=2),
            nn.ReLU(),
            nn.Conv1d(32, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(8)
        )
        
        # Actor
        self.actor = nn.Sequential(
            nn.Linear(4 + 16*8, 128),
            nn.Tanh(),
            nn.Linear(128, 128),
            nn.Tanh(),
            nn.Linear(128, 1)
        )
        
        # Critic
        self.critic = nn.Sequential(
            nn.Linear(4 + 16*8, 128),
            nn.Tanh(),
            nn.Linear(128, 128),
            nn.Tanh(),
            nn.Linear(128, 1)
        )
        
        # Initialize critic with small weights to avoid instability
        for m in self.critic.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=0.01)
                nn.init.constant_(m.bias, 0)
        
        self.log_std = nn.Parameter(torch.zeros(1) - 5.0)
    
    def forward(self, base_features, curv_sequence):
        curv_input = curv_sequence.unsqueeze(1)
        conv_out = self.conv(curv_input)
        conv_flat = conv_out.reshape(conv_out.size(0), -1)
        combined = torch.cat([base_features, conv_flat], dim=1)
        
        return self.actor(combined), self.log_std.exp(), self.critic(combined)

def load_bc_weights(ac, bc_path):
    ckpt = torch.load(bc_path, map_location='cpu', weights_only=False)
    bc_state = ckpt['model_state_dict']
    
    # Create new state dict with proper keys
    new_state = {}
    for k, v in ac.state_dict().items():
        if k.startswith('conv.'):
            # Load conv weights from BC
            if k in bc_state:
                new_state[k] = bc_state[k]
            else:
                new_state[k] = v
        elif k.startswith('actor.'):
            # Map mlp.* â†’ actor.*
            bc_key = 'mlp.' + k[6:]  # Remove 'actor.' prefix
            if bc_key in bc_state:
                new_state[k] = bc_state[bc_key]
            else:
                new_state[k] = v
        elif k == 'log_std':
            new_state[k] = v  # Keep initialized value
        else:
            # Critic stays randomly initialized
            new_state[k] = v
    
    ac.load_state_dict(new_state)
    print("âœ… Loaded BC weights into ActorCritic (conv + actor)")

class Ctrl:
    def __init__(self, ac, collect=False):
        self.ac, self.collect = ac, collect
        self.ei, self.pe = 0.0, 0.0
        self.states_base, self.states_curv, self.actions, self.rewards = [], [], [], []
        self.pl, self.sc = None, 0
    
    def update(self, target, current, state, future_plan):
        e = target - current
        self.ei += e
        ed = e - self.pe
        self.pe = e
        
        base = np.array([e, self.ei, ed, state.v_ego], dtype=np.float32)
        curvs = [(future_plan.lataccel[i] - state.roll_lataccel) / max(state.v_ego**2, 1.0) 
                 if i < len(future_plan.lataccel) else 0.0 for i in range(49)]
        curv_seq = np.array(curvs, dtype=np.float32)
        
        if self.collect:
            base_norm = torch.FloatTensor(base / BASE_SCALE).to(device)
            curv_norm = torch.FloatTensor(curv_seq / CURV_SCALE).to(device)
            
            with torch.no_grad():
                mean, std, _ = self.ac(base_norm.unsqueeze(0), curv_norm.unsqueeze(0))
                a = torch.distributions.Normal(mean, std).sample()
            
            lat_err = (target - current) ** 2 * 100
            r = 0.0
            if self.pl and self.sc >= CONTROL_START_IDX and self.sc < COST_END_IDX:
                jerk = ((current - self.pl) / DEL_T) ** 2 * 100
                r = -(lat_err * LAT_ACCEL_COST_MULTIPLIER + jerk) / 1000.0
            
            self.states_base.append(base_norm)
            self.states_curv.append(curv_norm)
            self.actions.append(a.squeeze(0))  # Remove batch dim only, keep action dim
            self.rewards.append(r)
            self.pl = current
            self.sc += 1
            return float(np.clip(a.item(), -2.0, 2.0))
        else:
            base_norm = torch.FloatTensor(base / BASE_SCALE).unsqueeze(0)
            curv_norm = torch.FloatTensor(curv_seq / CURV_SCALE).unsqueeze(0)
            with torch.no_grad():
                mean, _, _ = self.ac(base_norm, curv_norm)
            return float(np.clip(mean.item(), -2.0, 2.0))

def compute_advantages_per_episode(rewards, values, gamma, lam):
    T = len(rewards)
    advs, gae = [], 0.0
    vals_list = values.squeeze().tolist() + [0.0]
    for t in reversed(range(T)):
        delta = rewards[t] + gamma * vals_list[t+1] - vals_list[t]
        gae = delta + gamma * lam * gae
        advs.insert(0, gae)
    advs = torch.FloatTensor(advs).to(device)
    rets = advs + torch.FloatTensor(vals_list[:-1]).to(device)
    return advs, rets

class PPO:
    def __init__(self, ac):
        self.ac = ac
        self.opt = optim.Adam(ac.parameters(), lr=lr)
    
    def update(self, ctrls):
        all_base, all_curv, all_actions, all_old_logprobs, all_advs, all_rets = [], [], [], [], [], []
        
        for ctrl in ctrls:
            if len(ctrl.states_base) == 0:
                continue
            
            base = torch.stack(ctrl.states_base)
            curv = torch.stack(ctrl.states_curv)
            actions = torch.stack(ctrl.actions)
            
            # Check for NaN in collected data
            if torch.isnan(base).any():
                print(f"WARNING: NaN in base states!")
                continue
            if torch.isnan(curv).any():
                print(f"WARNING: NaN in curv states!")
                continue
            if torch.isnan(actions).any():
                print(f"WARNING: NaN in actions!")
                continue
            
            with torch.no_grad():
                means, stds, values = self.ac(base, curv)
                
                # Check for NaN in network outputs
                if torch.isnan(means).any():
                    print(f"WARNING: NaN in means during data collection!")
                    print(f"  base range: [{base.min():.3f}, {base.max():.3f}]")
                    print(f"  curv range: [{curv.min():.3f}, {curv.max():.3f}]")
                    continue
                
                dist = torch.distributions.Normal(means, stds)
                old_logprobs = dist.log_prob(actions).sum(-1)
                advs, rets = compute_advantages_per_episode(ctrl.rewards, values, gamma, gae_lambda)
            
            all_base.append(base)
            all_curv.append(curv)
            all_actions.append(actions)
            all_old_logprobs.append(old_logprobs)
            all_advs.append(advs)
            all_rets.append(rets)
        
        if len(all_base) == 0:
            return 0
        
        base = torch.cat(all_base)
        curv = torch.cat(all_curv)
        actions = torch.cat(all_actions)
        old_logprobs = torch.cat(all_old_logprobs)
        advs = torch.cat(all_advs)
        rets = torch.cat(all_rets)
        
        # Clip returns to prevent gradient explosion
        rets = torch.clamp(rets, -10, 10)
        
        advs = (advs - advs.mean()) / (advs.std() + 1e-8)
        
        dataset = TensorDataset(base, curv, actions, old_logprobs, advs, rets)
        
        total_loss, n = 0, 0
        for _ in range(K_epochs):
            for batch in DataLoader(dataset, batch_size=min(batch_size, len(base)), shuffle=True):
                bb, bc, ba, bolp, badv, bret = batch
                
                # Check inputs for NaN
                if torch.isnan(bb).any() or torch.isnan(bc).any():
                    print("ERROR: NaN in batch inputs!")
                    continue
                
                means, stds, values = self.ac(bb, bc)
                
                # Check outputs for NaN
                if torch.isnan(means).any():
                    print(f"ERROR: NaN in means after forward!")
                    print(f"  bb range: [{bb.min():.3f}, {bb.max():.3f}]")
                    print(f"  bc range: [{bc.min():.3f}, {bc.max():.3f}]")
                    # Check weights for NaN
                    for name, param in self.ac.named_parameters():
                        if torch.isnan(param).any():
                            print(f"    NaN in {name}!")
                    return 0
                dist = torch.distributions.Normal(means, stds)
                logp = dist.log_prob(ba).sum(-1)
                
                # Check for NaN in intermediate values
                if torch.isnan(logp).any():
                    print("ERROR: NaN in logp!")
                    print(f"  means range: [{means.min():.3f}, {means.max():.3f}]")
                    print(f"  stds: {stds.item():.6f}")
                    print(f"  ba range: [{ba.min():.3f}, {ba.max():.3f}]")
                    return 0
                
                ratios = torch.exp(logp - bolp)
                
                if torch.isnan(ratios).any() or torch.isinf(ratios).any():
                    print("ERROR: NaN/Inf in ratios!")
                    print(f"  logp range: [{logp.min():.3f}, {logp.max():.3f}]")
                    print(f"  bolp range: [{bolp.min():.3f}, {bolp.max():.3f}]")
                    print(f"  ratio range: [{ratios.min():.6f}, {ratios.max():.6f}]")
                    return 0
                
                aloss = -torch.min(ratios * badv, torch.clamp(ratios, 1-eps_clip, 1+eps_clip) * badv).mean()
                closs = F.mse_loss(values.squeeze(-1), bret)
                ent = dist.entropy().sum(-1).mean()
                loss = aloss + 0.5 * closs - entropy_coef * ent
                
                if torch.isnan(loss):
                    print(f"ERROR: NaN in loss!")
                    print(f"  aloss: {aloss.item()}")
                    print(f"  closs: {closs.item()}")
                    print(f"  ent: {ent.item()}")
                    return 0
                
                self.opt.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.ac.parameters(), max_norm=0.5)
                self.opt.step()
                
                total_loss += loss.item()
                n += 1
        
        return total_loss/n

def rollout(ac, f, collect=False):
    ctrl = Ctrl(ac, collect=collect)
    sim = TinyPhysicsSimulator(model_onnx, str(f), controller=ctrl)
    cost = sim.rollout()['total_cost']
    return ctrl if collect else None, cost

def evaluate(ac, files, n=10):
    return np.mean([rollout(ac, f, False)[1] for f in files[:n]])

def train():
    ac = ActorCritic().to(device)
    load_bc_weights(ac, Path('./experiments/exp023_conv/model.pth'))
    
    bc_cost = evaluate(ac, test_files)
    print(f"Conv BC init: {bc_cost:.2f}, Ïƒ={ac.log_std.exp().item():.4f}")
    print(f"Target: <45 (need {bc_cost - 45:.1f} improvement)\n")
    
    ppo = PPO(ac)
    best = bc_cost
    
    for ep in trange(max_epochs):
        ctrls, costs = [], []
        for f in random.sample(train_files, routes_per_epoch):
            c, cost = rollout(ac, f, collect=True)
            ctrls.append(c)
            costs.append(cost)
        
        loss = ppo.update(ctrls)
        
        if ep % eval_interval == 0:
            test_cost = evaluate(ac, test_files, 10)
            if test_cost < best:
                best = test_cost
                torch.save({'model_state_dict': ac.state_dict()}, 
                           'experiments/exp023_conv/model_ppo_best.pth')
            print(f"E{ep:2d} tr={np.mean(costs):5.1f} tst={test_cost:5.1f} best={best:5.1f} Ïƒ={ac.log_std.exp().item():.4f}", flush=True)
            
            if best < 45:
                print(f"\nðŸŽ¯ SOLVED! best={best:.2f} < 45")
                break
    
    print(f"\nâœ… Best: {best:.2f} (started at {bc_cost:.2f})")

if __name__ == '__main__':
    train()

